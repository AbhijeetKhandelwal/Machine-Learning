{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#setting the Keras backend as Tensorflow\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1048575, 2)\n",
      "   Sentiment                                      SentimentText\n",
      "0          0  is upset that he can't update his Facebook by ...\n",
      "1          0  @Kenichan I dived many times for the ball. Man...\n",
      "2          0    my whole body feels itchy and like its on fire \n",
      "3          0  @nationwideclass no, it's not behaving at all....\n",
      "4          0                      @Kwesidei not the whole crew \n"
     ]
    }
   ],
   "source": [
    "#function for data preprocessing\n",
    "def ingest():\n",
    "    data = pd.read_csv('C:\\\\Users\\\\bhumi\\\\Desktop\\\\repo\\\\NLP\\\\twitter_Sentiment_Train.csv', encoding = \"latin1\")\n",
    "    data = data[data.Sentiment.isnull() == False]\n",
    "    data['Sentiment'] = data['Sentiment'].map( {4:1, 0:0} )\n",
    "    #data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    data = data[data['SentimentText'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)\n",
    "    return data\n",
    "\n",
    "data = ingest()\n",
    "print(data.head(5))\n",
    "n=data.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar:   7%|███▉                                                      | 72311/1048575 [00:09<02:10, 7483.67it/s]C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "progress-bar: 100%|████████████████████████████████████████████████████████| 1048575/1048575 [02:17<00:00, 7602.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenizing function that splits each tweet into tokens and removes user mentions, hashtags and urls\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        #tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tweet = tweet.lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        return list(tokens)\n",
    "    except:\n",
    "        return 'NC'\n",
    "\n",
    "#The results of the tokenization should now be cleaned to remove lines with 'NC', resulting from a tokenization error\n",
    "def postprocess(data, n=1600000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data,n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the word2vec model\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),\n",
    "                                                    np.array(data.head(n).Sentiment), test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n",
      "838860it [00:08, 96070.35it/s] \n",
      "209715it [00:02, 77973.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledSentence(['i', 'stood', 'in', 'it'], ['TRAIN_0'])\n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 838860/838860 [00:00<00:00, 1162560.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 838860/838860 [00:00<00:00, 1169051.23it/s]\n",
      "C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00661612  1.60322595 -0.0285238  -0.29310158  2.27271032  0.23961651\n",
      " -0.85697293 -0.78719872  0.1139839   1.61173224 -0.78645813  0.17944191\n",
      " -2.2367816  -0.99291098  1.42722046 -0.08854914  0.50589973  1.30507839\n",
      "  0.64027238  1.20982063  1.66881382  0.17526485 -1.12502837  0.5388692\n",
      " -0.50227892  0.95561224 -1.06664431 -1.23184681 -1.95974958  1.59780836\n",
      " -1.23589766 -0.41052991  1.28144324 -1.20835114 -3.21540833 -1.03669298\n",
      " -1.35284543  0.1136034  -2.06808567 -0.92073929  2.38989377 -0.32028002\n",
      " -0.34214631 -0.60228592  0.66093701 -1.2007153  -1.37734532 -0.71835512\n",
      "  0.4950093   1.17118609  0.14439274  0.97243285  1.53685832  0.26616859\n",
      "  0.93904871 -0.268231   -0.32518822 -0.96176201 -1.3345896  -1.07238901\n",
      "  1.33539546 -0.45341572  2.51974535  1.22182751 -0.52450651  0.55051321\n",
      "  0.08234909 -0.38503018 -1.8710115   0.99669969  1.24302304 -1.09327352\n",
      " -0.51814073  0.54013836 -0.10833254 -0.8102088  -2.18693709  0.84348094\n",
      "  0.32521605  0.15980409  1.76375842  2.39031863  0.75851399 -0.18266399\n",
      " -0.33322322  3.21837783 -1.45587075 -0.21021974  0.59215611 -0.86462575\n",
      " -0.042805    0.40707311  2.14186764  0.16987482 -1.38506985 -0.50833625\n",
      " -0.35686889  0.37566534  0.15573898  0.22704291 -0.98312438  0.21101214\n",
      " -1.27106178  1.06310081  0.85306174  0.08670176  0.17952597  0.47637767\n",
      " -1.02675843  0.51194978 -2.37532711  0.22917704  0.8135494   2.08035564\n",
      "  0.87171447  1.86752474  0.48206192  1.73146844  0.46094647 -0.62487584\n",
      "  1.05340624  1.88993979 -0.11255556 -0.31469983  2.09091115  0.60983616\n",
      "  2.01085401 -1.35133314 -0.89611387  1.63814485  1.30994773 -1.26406908\n",
      "  0.13542497 -0.22997621  1.51489604 -2.09768414  0.14203893 -0.34592214\n",
      "  1.18506229  1.3491509  -0.84437984  1.4295969  -1.53105295 -0.54411483\n",
      " -0.80584657 -1.18278611  1.85547578 -1.14505363  0.99344742  1.16581154\n",
      " -2.21780372 -0.51023263 -0.44109461  0.09532905  0.48239678  0.26678494\n",
      " -1.07891059 -0.04089502  0.63685769 -1.68985152 -1.07036912 -0.30458656\n",
      " -1.65100074 -1.24733019 -0.55795628 -0.13774419 -2.8226943  -1.01657355\n",
      " -0.38250539  0.04354307  0.71061617 -2.20641828 -0.14347835 -2.3432169\n",
      "  1.59885359  0.56511247 -0.03805664 -0.2531144  -0.34378225 -1.74841416\n",
      " -0.43987125 -1.62321138 -1.91051924  0.87831855  0.9185378  -0.16401903\n",
      " -0.93308878 -0.24562453 -1.05525684 -1.23514569  1.05933261 -0.91207033\n",
      " -1.98179281 -0.79813159 -0.41452736 -1.73142052 -3.10613155 -0.12000484\n",
      "  0.39940616  0.63387227]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('goood', 0.7170765399932861), ('great', 0.700944721698761), ('pleasant', 0.6300460696220398), ('tough', 0.6253600120544434), ('nice', 0.621490478515625), ('fantastic', 0.6195719838142395), ('gd', 0.6188822984695435), ('rough', 0.618777871131897), ('gud', 0.6020275354385376), ('brilliant', 0.6000094413757324)]\n"
     ]
    }
   ],
   "source": [
    "#Building word2vec of 200 Dimension\n",
    "n_dim=200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)],total_examples=tweet_w2v.corpus_count,epochs=tweet_w2v.iter)\n",
    "#test built word2vec model\n",
    "print(tweet_w2v['good'])\n",
    "print(tweet_w2v.most_similar('good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 23938\n"
     ]
    }
   ],
   "source": [
    "print('building tf-idf matrix ...')\n",
    "#buliding word to ID mapping\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train combines word_vectors with tf-idf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "39099it [00:09, 3933.83it/s]C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "838860it [03:39, 3817.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_vecs_w2v shape (838860, 200)\n",
      "building test combines word_vectors with tf-idf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209715it [00:56, 3699.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_vecs_w2v shape (209715, 200)\n"
     ]
    }
   ],
   "source": [
    "#training Word2vec\n",
    "from sklearn.preprocessing import scale\n",
    "print('building train combines word_vectors with tf-idf ...')\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "print('train_vecs_w2v shape', train_vecs_w2v.shape)\n",
    "print('building test combines word_vectors with tf-idf ...')\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "print('test_vecs_w2v shape', test_vecs_w2v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to train DNN model for sentiment analysis...\n",
      "Epoch 1/100\n",
      " - 11s - loss: 0.3795 - acc: 0.8348\n",
      "Epoch 2/100\n",
      " - 10s - loss: 0.3603 - acc: 0.8441\n",
      "Epoch 3/100\n",
      " - 10s - loss: 0.3553 - acc: 0.8468\n",
      "Epoch 4/100\n",
      " - 10s - loss: 0.3522 - acc: 0.8479\n",
      "Epoch 5/100\n",
      " - 11s - loss: 0.3498 - acc: 0.8490\n",
      "Epoch 6/100\n",
      " - 10s - loss: 0.3481 - acc: 0.8499\n",
      "Epoch 7/100\n",
      " - 10s - loss: 0.3467 - acc: 0.8505\n",
      "Epoch 8/100\n",
      " - 10s - loss: 0.3456 - acc: 0.8511\n",
      "Epoch 9/100\n",
      " - 10s - loss: 0.3446 - acc: 0.8515\n",
      "Epoch 10/100\n",
      " - 10s - loss: 0.3437 - acc: 0.8517\n",
      "Epoch 11/100\n",
      " - 11s - loss: 0.3429 - acc: 0.8522\n",
      "Epoch 12/100\n",
      " - 11s - loss: 0.3423 - acc: 0.8527\n",
      "Epoch 13/100\n",
      " - 11s - loss: 0.3417 - acc: 0.8528\n",
      "Epoch 14/100\n",
      " - 11s - loss: 0.3412 - acc: 0.8532\n",
      "Epoch 15/100\n",
      " - 10s - loss: 0.3408 - acc: 0.8534\n",
      "Epoch 16/100\n",
      " - 11s - loss: 0.3403 - acc: 0.8535\n",
      "Epoch 17/100\n",
      " - 10s - loss: 0.3398 - acc: 0.8538\n",
      "Epoch 18/100\n",
      " - 10s - loss: 0.3395 - acc: 0.8540\n",
      "Epoch 19/100\n",
      " - 11s - loss: 0.3392 - acc: 0.8539\n",
      "Epoch 20/100\n",
      " - 11s - loss: 0.3387 - acc: 0.8542\n",
      "Epoch 21/100\n",
      " - 11s - loss: 0.3385 - acc: 0.8545\n",
      "Epoch 22/100\n",
      " - 11s - loss: 0.3383 - acc: 0.8543\n",
      "Epoch 23/100\n",
      " - 11s - loss: 0.3380 - acc: 0.8543\n",
      "Epoch 24/100\n",
      " - 10s - loss: 0.3378 - acc: 0.8546\n",
      "Epoch 25/100\n",
      " - 10s - loss: 0.3375 - acc: 0.8548\n",
      "Epoch 26/100\n",
      " - 10s - loss: 0.3372 - acc: 0.8550\n",
      "Epoch 27/100\n",
      " - 10s - loss: 0.3370 - acc: 0.8551\n",
      "Epoch 28/100\n",
      " - 10s - loss: 0.3369 - acc: 0.8554\n",
      "Epoch 29/100\n",
      " - 10s - loss: 0.3366 - acc: 0.8553\n",
      "Epoch 30/100\n",
      " - 10s - loss: 0.3365 - acc: 0.8553\n",
      "Epoch 31/100\n",
      " - 10s - loss: 0.3362 - acc: 0.8553\n",
      "Epoch 32/100\n",
      " - 11s - loss: 0.3362 - acc: 0.8555\n",
      "Epoch 33/100\n",
      " - 11s - loss: 0.3359 - acc: 0.8556\n",
      "Epoch 34/100\n",
      " - 11s - loss: 0.3358 - acc: 0.8555\n",
      "Epoch 35/100\n",
      " - 10s - loss: 0.3357 - acc: 0.8559\n",
      "Epoch 36/100\n",
      " - 10s - loss: 0.3355 - acc: 0.8558\n",
      "Epoch 37/100\n",
      " - 10s - loss: 0.3352 - acc: 0.8558\n",
      "Epoch 38/100\n",
      " - 10s - loss: 0.3351 - acc: 0.8559\n",
      "Epoch 39/100\n",
      " - 10s - loss: 0.3350 - acc: 0.8557\n",
      "Epoch 40/100\n",
      " - 11s - loss: 0.3349 - acc: 0.8561\n",
      "Epoch 41/100\n",
      " - 10s - loss: 0.3348 - acc: 0.8560\n",
      "Epoch 42/100\n",
      " - 10s - loss: 0.3347 - acc: 0.8562\n",
      "Epoch 43/100\n",
      " - 10s - loss: 0.3346 - acc: 0.8562\n",
      "Epoch 44/100\n",
      " - 12s - loss: 0.3345 - acc: 0.8561\n",
      "Epoch 45/100\n",
      " - 10s - loss: 0.3344 - acc: 0.8563\n",
      "Epoch 46/100\n",
      " - 10s - loss: 0.3343 - acc: 0.8562\n",
      "Epoch 47/100\n",
      " - 11s - loss: 0.3342 - acc: 0.8564\n",
      "Epoch 48/100\n",
      " - 11s - loss: 0.3341 - acc: 0.8565\n",
      "Epoch 49/100\n",
      " - 10s - loss: 0.3339 - acc: 0.8566\n",
      "Epoch 50/100\n",
      " - 10s - loss: 0.3339 - acc: 0.8566\n",
      "Epoch 51/100\n",
      " - 10s - loss: 0.3338 - acc: 0.8565\n",
      "Epoch 52/100\n",
      " - 10s - loss: 0.3337 - acc: 0.8566\n",
      "Epoch 53/100\n",
      " - 10s - loss: 0.3337 - acc: 0.8564\n",
      "Epoch 54/100\n",
      " - 10s - loss: 0.3336 - acc: 0.8566\n",
      "Epoch 55/100\n",
      " - 10s - loss: 0.3335 - acc: 0.8566\n",
      "Epoch 56/100\n",
      " - 11s - loss: 0.3334 - acc: 0.8568\n",
      "Epoch 57/100\n",
      " - 11s - loss: 0.3334 - acc: 0.8567\n",
      "Epoch 58/100\n",
      " - 10s - loss: 0.3333 - acc: 0.8567\n",
      "Epoch 59/100\n",
      " - 11s - loss: 0.3333 - acc: 0.8570\n",
      "Epoch 60/100\n",
      " - 10s - loss: 0.3332 - acc: 0.8569\n",
      "Epoch 61/100\n",
      " - 11s - loss: 0.3330 - acc: 0.8570\n",
      "Epoch 62/100\n",
      " - 11s - loss: 0.3330 - acc: 0.8568\n",
      "Epoch 63/100\n",
      " - 11s - loss: 0.3330 - acc: 0.8569\n",
      "Epoch 64/100\n",
      " - 11s - loss: 0.3329 - acc: 0.8571\n",
      "Epoch 65/100\n",
      " - 11s - loss: 0.3329 - acc: 0.8569\n",
      "Epoch 66/100\n",
      " - 10s - loss: 0.3328 - acc: 0.8569\n",
      "Epoch 67/100\n",
      " - 10s - loss: 0.3328 - acc: 0.8570\n",
      "Epoch 68/100\n",
      " - 10s - loss: 0.3326 - acc: 0.8571\n",
      "Epoch 69/100\n",
      " - 10s - loss: 0.3326 - acc: 0.8569\n",
      "Epoch 70/100\n",
      " - 10s - loss: 0.3326 - acc: 0.8571\n",
      "Epoch 71/100\n",
      " - 11s - loss: 0.3324 - acc: 0.8572\n",
      "Epoch 72/100\n",
      " - 10s - loss: 0.3324 - acc: 0.8572\n",
      "Epoch 73/100\n",
      " - 11s - loss: 0.3324 - acc: 0.8571\n",
      "Epoch 74/100\n",
      " - 11s - loss: 0.3323 - acc: 0.8572\n",
      "Epoch 75/100\n",
      " - 11s - loss: 0.3323 - acc: 0.8575\n",
      "Epoch 76/100\n",
      " - 11s - loss: 0.3321 - acc: 0.8574\n",
      "Epoch 77/100\n",
      " - 11s - loss: 0.3322 - acc: 0.8573\n",
      "Epoch 78/100\n",
      " - 11s - loss: 0.3321 - acc: 0.8572\n",
      "Epoch 79/100\n",
      " - 10s - loss: 0.3320 - acc: 0.8576\n",
      "Epoch 80/100\n",
      " - 11s - loss: 0.3322 - acc: 0.8573\n",
      "Epoch 81/100\n",
      " - 10s - loss: 0.3320 - acc: 0.8572\n",
      "Epoch 82/100\n",
      " - 11s - loss: 0.3319 - acc: 0.8575\n",
      "Epoch 83/100\n",
      " - 11s - loss: 0.3319 - acc: 0.8574\n",
      "Epoch 84/100\n",
      " - 11s - loss: 0.3318 - acc: 0.8573\n",
      "Epoch 85/100\n",
      " - 11s - loss: 0.3318 - acc: 0.8574\n",
      "Epoch 86/100\n",
      " - 11s - loss: 0.3318 - acc: 0.8578\n",
      "Epoch 87/100\n",
      " - 11s - loss: 0.3317 - acc: 0.8574\n",
      "Epoch 88/100\n",
      " - 11s - loss: 0.3316 - acc: 0.8575\n",
      "Epoch 89/100\n",
      " - 11s - loss: 0.3317 - acc: 0.8573\n",
      "Epoch 90/100\n",
      " - 11s - loss: 0.3316 - acc: 0.8577\n",
      "Epoch 91/100\n",
      " - 11s - loss: 0.3316 - acc: 0.8575\n",
      "Epoch 92/100\n",
      " - 11s - loss: 0.3316 - acc: 0.8576\n",
      "Epoch 93/100\n",
      " - 11s - loss: 0.3314 - acc: 0.8575\n",
      "Epoch 94/100\n",
      " - 11s - loss: 0.3316 - acc: 0.8576\n",
      "Epoch 95/100\n",
      " - 11s - loss: 0.3315 - acc: 0.8575\n",
      "Epoch 96/100\n",
      " - 11s - loss: 0.3314 - acc: 0.8576\n",
      "Epoch 97/100\n",
      " - 11s - loss: 0.3315 - acc: 0.8575\n",
      "Epoch 98/100\n",
      " - 11s - loss: 0.3313 - acc: 0.8576\n",
      "Epoch 99/100\n",
      " - 11s - loss: 0.3314 - acc: 0.8577\n",
      "Epoch 100/100\n",
      " - 11s - loss: 0.3313 - acc: 0.8577\n",
      "Evaluate trained model on test dataset...\n",
      "Accuracy:  0.849743699786\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input,  Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print('begin to train DNN model for sentiment analysis...')\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=100, batch_size=256, verbose=2)\n",
    "\n",
    "print('Evaluate trained model on test dataset...')\n",
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=256, verbose=2)\n",
    "print('Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
