{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\bhumi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.8487Epoch 00001: loss improved from inf to 2.84781, saving model to weights-improvement-01-2.8478.hdf5\n",
      "19809/19809 [==============================] - 132s 7ms/step - loss: 2.8478\n",
      "Epoch 2/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.7979Epoch 00002: loss improved from 2.84781 to 2.79780, saving model to weights-improvement-02-2.7978.hdf5\n",
      "19809/19809 [==============================] - 139s 7ms/step - loss: 2.7978\n",
      "Epoch 3/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.6853Epoch 00003: loss improved from 2.79780 to 2.68527, saving model to weights-improvement-03-2.6853.hdf5\n",
      "19809/19809 [==============================] - 128s 6ms/step - loss: 2.6853\n",
      "Epoch 4/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.5149Epoch 00004: loss improved from 2.68527 to 2.51479, saving model to weights-improvement-04-2.5148.hdf5\n",
      "19809/19809 [==============================] - 126s 6ms/step - loss: 2.5148\n",
      "Epoch 5/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.4444Epoch 00005: loss improved from 2.51479 to 2.44323, saving model to weights-improvement-05-2.4432.hdf5\n",
      "19809/19809 [==============================] - 126s 6ms/step - loss: 2.4432\n",
      "Epoch 6/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.4006Epoch 00006: loss improved from 2.44323 to 2.39939, saving model to weights-improvement-06-2.3994.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.3994\n",
      "Epoch 7/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.3720Epoch 00007: loss improved from 2.39939 to 2.37202, saving model to weights-improvement-07-2.3720.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.3720\n",
      "Epoch 8/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.3433Epoch 00008: loss improved from 2.37202 to 2.34348, saving model to weights-improvement-08-2.3435.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.3435\n",
      "Epoch 9/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.3149Epoch 00009: loss improved from 2.34348 to 2.31510, saving model to weights-improvement-09-2.3151.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.3151\n",
      "Epoch 10/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.2906Epoch 00010: loss improved from 2.31510 to 2.29149, saving model to weights-improvement-10-2.2915.hdf5\n",
      "19809/19809 [==============================] - 126s 6ms/step - loss: 2.2915\n",
      "Epoch 11/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.2700Epoch 00011: loss improved from 2.29149 to 2.27031, saving model to weights-improvement-11-2.2703.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.2703\n",
      "Epoch 12/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.2496Epoch 00012: loss improved from 2.27031 to 2.24967, saving model to weights-improvement-12-2.2497.hdf5\n",
      "19809/19809 [==============================] - 125s 6ms/step - loss: 2.2497\n",
      "Epoch 13/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.2311Epoch 00013: loss improved from 2.24967 to 2.23129, saving model to weights-improvement-13-2.2313.hdf5\n",
      "19809/19809 [==============================] - 130s 7ms/step - loss: 2.2313\n",
      "Epoch 14/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.2073Epoch 00014: loss improved from 2.23129 to 2.20642, saving model to weights-improvement-14-2.2064.hdf5\n",
      "19809/19809 [==============================] - 133s 7ms/step - loss: 2.2064\n",
      "Epoch 15/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.1883Epoch 00015: loss improved from 2.20642 to 2.18785, saving model to weights-improvement-15-2.1879.hdf5\n",
      "19809/19809 [==============================] - 132s 7ms/step - loss: 2.1879\n",
      "Epoch 16/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.1632Epoch 00016: loss improved from 2.18785 to 2.16427, saving model to weights-improvement-16-2.1643.hdf5\n",
      "19809/19809 [==============================] - 126s 6ms/step - loss: 2.1643\n",
      "Epoch 17/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.1416Epoch 00017: loss improved from 2.16427 to 2.14119, saving model to weights-improvement-17-2.1412.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.1412\n",
      "Epoch 18/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.1134Epoch 00018: loss improved from 2.14119 to 2.11279, saving model to weights-improvement-18-2.1128.hdf5\n",
      "19809/19809 [==============================] - 134s 7ms/step - loss: 2.1128\n",
      "Epoch 19/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.0816Epoch 00019: loss improved from 2.11279 to 2.08093, saving model to weights-improvement-19-2.0809.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.0809\n",
      "Epoch 20/20\n",
      "19712/19809 [============================>.] - ETA: 0s - loss: 2.0527Epoch 00020: loss improved from 2.08093 to 2.05359, saving model to weights-improvement-20-2.0536.hdf5\n",
      "19809/19809 [==============================] - 127s 6ms/step - loss: 2.0536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16dcd2df748>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small LSTM Network to Generate Text\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"C:\\\\Users\\\\bhumi\\\\Desktop\\\\repo\\\\Caption_generator\\\\Shaks.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "#print \"Total Characters: \", n_chars\n",
    "#print \"Total Vocab: \", n_vocab\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "#print \"Total Patterns: \", n_patterns\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"C:\\\\Users\\\\bhumi\\\\weights-improvement-19-2.0809.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" \n",
      "orosaurus\n",
      "orthogoniosaurus\n",
      "orthomerus\n",
      "oryctodromeus\n",
      "oshanosaurus\n",
      "osmakasaurus\n",
      "ostafrikasaurus\n",
      "ostro \"\n",
      "oaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaurus\n",
      "suooooaur\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
